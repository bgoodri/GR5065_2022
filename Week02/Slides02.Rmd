---
title: Probability with Discrete Variables
author: Ben Goodrich
date: "`r format(Sys.time(), '%B %d, %Y')`"
autosize: true
header-includes:
   - \usepackage[makeroom]{cancel}
   - \usepackage{amsmath}
   - \usepackage{color}
output:
  ioslides_presentation:
    widescreen: true
editor_options: 
  chunk_output_type: console
---
<style type="text/css">
slides > slide:not(.nobackground):after {
  content: '';
}
</style>


```{r, setup, include = FALSE}
options(width = 90)
library(knitr)
knit_hooks$set(small.mar = function(before, options, envir) {
    if (before) par(mar = c(4, 4, .1, .1))  # smaller margin on top and right
})
```

## Review of Last Week

* For Frequentists like Fisher and Neyman, probability is needed to describe the consequences 
  of explicit randomization:
  
    * Random sampling of units from a much larger population
    * Random assignment of units to treatment or control groups
    * Random measurement or other physical error
    
* Supervised learning usually randomizes which observations are put in the training 
  or testing set but mostly does not use probability to describe the implications
  of doing so and thus cannot characterize uncertainty
  
* Bayesians disagree sharply with both of these schools of thought

    * Probability is fundamental for science because we need to describe all forms of
    uncertainty, whether due to overt randomization or not

## Random Variables (R.V.) {.build}

- A function is a rule that UNIQUELY maps each element of an input set to some element of an output set, e.g. $e^x$ maps real numbers $\left(\mathbb{R}\right)$ to 
non-negative real numbers $\left(\mathbb{R_+}\right)$
- A random variable is a FUNCTION from the sample space, $\Omega$, to some subset of $\mathbb{R}$ with a probability-based rule
- If $\Omega$ is discrete with a finite number of elements, then we can simply 
  enumerate an equivalent number of probabilities to define a random variable
```{r}
die  <- 1:6 # Omega in this case
roll <- sample(die, size = 1, prob = rep(1 / 6, times = 6)) # code calls a function
# roll is a realization of the function
```
- Do not conflate a REALIZATION of a R.V. with the FUNCTION that generated it
- By convention, a capital letter, $X$, indicates a R.V.
and its lower-case counterpart, $x$, indicates a realization of $X$

## Basics of Bowling

Each "frame" in bowling starts with $n = 10$ pins & you get up to 2 rolls per frame
```{r, echo = FALSE}
vembedr::embed_url("https://youtu.be/HeiNrSllyzA?t=05")
```

## Approaching Bowling Probabilistically {.build}

> - What is $\Omega$ for your first roll of a frame of bowling?
> - If $b^p = x$, then $\log_b\left(x\right) = p$. Let the probability of knocking down 
  $X$ out of $n$ pins be given by a form of [Benford's Law](https://en.wikipedia.org/wiki/Benford%27s_law):
  $\Pr\left(x \mid n\right) = \log_{n + 2}\left(1 + \frac{1}{n + 1 - x}\right)$
```{r, Pr}
# probability of knocking down x out of n pins
Pr <- function(x, n = 10) ifelse(x > n, 0, log(1 + 1 / (n + 1 - x), base = n + 2))
Omega <- 0:10 # 0, 1, ..., 10
names(Omega) <- as.character(Omega)
source("bowling.R") # does the above

x <- sample(Omega, size = 1, prob = Pr(Omega)) # realization of bowling random variable
round(c(Pr(Omega), total = sum(Pr(Omega))), digits = 5)
```

## Second Roll in a Frame of Bowling

> - How would you compute the probability of knocking down all the remaining pins on 
  your second roll?
> - Let $X_{1}$ and $X_{2}$ respectively be the number of pins knocked down on 
  the first and second rolls of a frame of bowling. What function yields the
  probability of knocking down $x_2$ pins on your second roll?

> - $\Pr\left(x_{2} \mid n = 10 - x_1\right) = 
\log_{10 - x_1 + 2}\left(1 + \frac{1}{10 - x_1 + 1 - x_2}\right) \times
\mathbb{I}\left\{x_{2}\leq 10 - x_{1}\right\}$
> - $\mathbb{I}\left\{\cdot\right\}$ is an "indicator function" that equals $1$ if it is true and $0$ if it is false
> - $\Pr\left(x_{2}\mid n = 10 - x_1\right)$ is a CONDITIONAL probability because it depends 
  (via $n$) on the realization of $x_1$

## From [Aristotelian Logic](https://en.wikipedia.org/wiki/Boolean_algebra) to Bivariate Probability

- In R, `TRUE` maps to $1$ and `FALSE` maps to $0$ when doing arithmetic operations
```{r, AND}
c(TRUE & TRUE, TRUE & FALSE, FALSE & FALSE)
c(TRUE * TRUE, TRUE * FALSE, FALSE * FALSE)
```
- Can generalize to probabilities on the $[0,1]$ interval to compute the probability
  that two (or more) propositions are true simultaneously
- $\bigcap$ reads as "and". __General Multiplication Rule__: $\Pr\left(A\bigcap B\right)=\Pr\left(B\right)\times\Pr\left(A\mid|B\right)=\Pr\left(A\right)\times\Pr\left(B\mid|A\right)$
  
## Independence

- Loosely, $A$ and $B$ are independent propositions if $A$ being true or false tells
  us nothing about the probability that $B$ is true (and vice versa)
- Formally, $A$ and $B$ are independent iff $\Pr\left(\left.A\right|B\right)=\Pr\left(A\right)$
  (and $\Pr\left(\left.B\right|A\right)=\Pr\left(B\right)$). Thus, 
  $\Pr\left(A\bigcap B\right)=\Pr\left(A\right)\times\Pr\left(B\right)$.
- Why is it reasonable to think
    - Two rolls in the same frame are not independent?
    - Two rolls in different frames are independent?
    - Rolls by two different people are independent regardless of whether they are in the same frame?

> - What is the probability of obtaining a turkey (3 consecutive strikes)?
> - What is the probability of knocking down $x$ pins on the first roll and $10 - x$ pins 
  on the second roll?
  
## Joint Probability of Two Rolls in Bowling

- How to obtain the joint probability, $\Pr\left(x_{1}\bigcap x_{2}\mid n=10\right)$, in general?

$$\Pr\left(x_1 \bigcap x_2 \mid n = 10\right) =  
\Pr\left(x_1 \mid n = 10\right) \times \Pr\left(x_{2}\mid n = 10 - x_1 \right) = $$
$$\log_{12}\left(1 + \frac{1}{11 - x_1}\right) \times
 \log_{12 - x_1}\left(1 + \frac{1}{11 - x_1 - x_2}\right) \times
\mathbb{I}\{x_1 + x_2 \leq 10\}$$

```{r, joint}
joint_Pr <- matrix(0, nrow = 11, ncol = 11, dimnames = list(Omega, Omega))
for (x_1 in Omega) { # joint_Pr is already filled in by source("bowling.R")
  Pr_x_1 <- Pr(x_1, n = 10)
  for (x_2 in 0:(10 - x_1))
    joint_Pr[x_1 + 1, x_2 + 1] <- Pr_x_1 * Pr(x_2, n = 10 - x_1)
}
sum(joint_Pr) # that sums to 1, can do View(joint_Pr) to see it better than on the next slide
```

## `joint_Pr`: Row is roll 1, Column is roll 2 {.smaller}

```{r, size='footnotesize', echo = FALSE, message = FALSE}
library(knitr)
library(kableExtra)
library(dplyr)
options("kableExtra.html.bsTable" = TRUE)
options(scipen = 5)
options(knitr.kable.NA = "")
tmp <- as.data.frame(joint_Pr)
for (i in 1:ncol(tmp)) 
  tmp[,i] <- cell_spec(round(tmp[,i], digits = 6), "html", bold = tmp[,i] == 0,
                       color = ifelse(tmp[,i] == 0, "red", "black"))
kable(tmp, digits = 5, align = 'c', escape = FALSE) %>%
    kable_styling("striped", full_width = FALSE)
```

## Aristotelian Logic to Probability of Alternatives

```{r, OR}
c(TRUE | FALSE, FALSE | FALSE, TRUE | TRUE)
c(TRUE + FALSE, FALSE + FALSE, TRUE + TRUE - TRUE * TRUE)
```

- What is the probability you fail to get a strike on this frame or the next one?

> - Can generalize Aristotelian logic to probabilities on the $[0,1]$ interval to compute the probability
  that one of two (or more) propositions is true
> - $\bigcup$ is read as "or". __General Addition Rule__: $\Pr\left(A\bigcup B\right)=\Pr\left(A\right)+\Pr\left(B\right)-\Pr\left(A\bigcap B\right)$
> - Iff $\Pr\left(A\bigcap B\right) = 0$, $A$ and $B$ are mutually exclusive (disjoint)

## What is the probability that $X_2 = 9$? {.smaller}

```{r, size='footnotesize', echo = FALSE, message = FALSE}
tmp <- as.data.frame(joint_Pr)
for (i in 1:ncol(tmp)) 
  tmp[,i] <- cell_spec(round(tmp[,i], digits = 6), "html", bold = tmp[,i] == 0,
                       color = ifelse(tmp[,i] == 0, "red", "black"))
kable(tmp, digits = 5, align = 'c', escape = FALSE) %>%
    kable_styling("striped", full_width = FALSE)
```

## Marginal Probabilities from Joint Probabilities {.smaller}

```{r, size='footnotesize', echo = FALSE, message = FALSE}
tmp <- as.data.frame(cbind(joint_Pr, " " = -1, "row-sum" = rowSums(joint_Pr)))
tmp <- rbind(tmp, " " = -1, "col-sum" = colSums(tmp))
for (i in 1:ncol(tmp)) 
  tmp[,i] <- cell_spec(round(tmp[,i], digits = 6), "html", 
                       bold = tmp[,i] == 0,
                       color = ifelse(tmp[,i] == 0, "red", 
                                      ifelse(tmp[,i] > 1 - 1e-8 | tmp[,i] < 0, 
                                             "white", "black")))
kable(tmp, digits = 5, align = 'c', escape = FALSE) %>%
    kable_styling("striped", full_width = FALSE)
```

## Marginal Distribution of Second Roll in Bowling

- How to obtain $\Pr\left(x_2\right)$ irrespective of $x_1$?
- Since events in the first roll are mutually exclusive, use the simplified
form of the General Addition Rule to "marginalize":
$$\begin{eqnarray*}
\Pr\left(x_{2}\right) & = & 
\sum_{x = 0}^{10}\Pr\left(X_1 = x\bigcap X_2 = x_{2}\mid n = 10\right)\\
 & = & \sum_{x = 0}^{10}
 \Pr\left(x\mid n = 10\right) \times \Pr\left(x_{2}\mid n = 10 - x\right)
\end{eqnarray*}$$
- This corresponds to a column-wise sum of `joint_Pr`

<script type="text/x-mathjax-config">
MathJax.Hub.Register.StartupHook("TeX Jax Ready",function () {
  MathJax.Hub.Insert(MathJax.InputJax.TeX.Definitions.macros,{
    cancel: ["Extension","cancel"],
    bcancel: ["Extension","cancel"],
    xcancel: ["Extension","cancel"],
    cancelto: ["Extension","cancel"]
  });
});
</script>

> - Better notation might be $\Pr\left( \cancel{x_1} \bigcap x_2 \mid n = 10\right)$

## Marginal, Conditional, and Joint Probabilities

> - To compose a joint (in this case, bivariate) probability, MULTIPLY a marginal probability by
  a conditional probability
> - To decompose a joint (in this case, bivariate) probability, ADD the relevant joint probabilities
  to obtain a marginal probability
> - To obtain a conditional probability, DIVIDE the joint probability (in this case, bivariate) by 
  the marginal probability of the event that you want to condition on because
$$\Pr\left(A\bigcap B\right)=\Pr\left(B\right)\times\Pr\left(A \mid B\right) =
\Pr\left(A\right)\times\Pr\left(B\mid A\right) \implies$$
$$\Pr\left(A \mid B\right)= \frac{\Pr\left(A\right)\times\Pr\left(B \mid A\right)}{\Pr\left(B\right)} 
\mbox{ if } \Pr\left(B\right) > 0$$
> - This is Bayes' Rule  
> - What is an expression for $\Pr\left(X_1 = 3 \mid X_2 = 4\right)$ in bowling?

## Conditioning on $X_2 = 4$ in Bowling {.smaller}

```{r, size='footnotesize', echo = FALSE}
tmp <- as.data.frame(joint_Pr)
for (i in 1:ncol(tmp)) 
  tmp[,i] <- cell_spec(round(tmp[,i], digits = 6), "html", bold = tmp[,i] == 0,
                       color = ifelse(tmp[,i] == 0, "red", 
                                      ifelse(i == 5, "black", "blue")))
kable(tmp, digits = 5, align = 'c', escape = FALSE) %>%
    kable_styling("striped", full_width = FALSE)
```

## $\Pr\left(X_1 = 3 \mid X_2 = 4\right)$ by Simulation vs. Bayes' Rule {.build}

```{r, sims, cache = TRUE, message = FALSE}
library(dplyr)
S <- 10^7 # practically infinite
tibble(x_1 = sample(Omega, size = S, replace = TRUE, prob = Pr(Omega)),
       x_2 = vapply(x_1, FUN.VALUE = integer(1), FUN = function(x) {
         # draw from conditional probability distribution given each value of x_1
         sample(Omega, size = 1, prob = Pr(Omega, n = 10 - x))
       })) %>%
filter(x_2 == 4) %>%
summarize(simulated = mean(x_1 == 3)) %>%
mutate(exact = joint_Pr["3", "4"] / sum(joint_Pr[ , "4"])) %>%
print(digits = 4)
```

## Bayesian vs Frequentist Probability

- Bayesians generalize this by taking $A$ to be "beliefs about whatever you do not know" and $B$ to be whatever you do know in 
$$\Pr\left(\left.A\right|B\right)= \frac{\Pr\left(A\right)\times\Pr\left(\left.B\right|A\right)}{\Pr\left(B\right)} \mbox{ if } \Pr\left(B\right) > 0$$
- Frequentists accept the validity Bayes' Rule but object to using the language of probability to describe 
  beliefs about unknown propositions and insist that probability is a property of a process 
  that can be defined as a limit
$$\Pr\left(A\right) = \lim_{S\uparrow\infty} 
\frac{\mbox{times that } A \mbox{ occurs in } S \mbox{ independent randomizations}}{S}$$

## Probability that a Huge Odd Integer is Prime

> - John Cook [asks](https://www.johndcook.com/blog/2010/10/06/probability-a-number-is-prime/)
  an interesting question: What is the probability $x$ is prime, where $x$ is a huge, odd integer
  like $1 + 10^{100,000,000}$?
    
> - To Frequentists, $x$ is not a random variable. It is either prime or composite and it makes no
  sense to say that it is "probably (not) prime"
> - To Bayesians, $x$ is either prime or composite but no one knows for sure which
> - The prime number theorem implies provides a way to choose the prior probability that
  $x$ is prime based on its number of digits, $\left(d\right)$
$$\Pr\left(x \mbox{ is prime} \mid d\right) = \frac{1}{d \ln 10} \approx \frac{1}{2.3 \times 10^{10}}$$
  although you could double that by taking into account that $x$ is odd

> - What is the probability that $\beta > 0$ in a regression model?

## Expectation of a Discrete Random Variable {.build}

```{r}
round(Pr(Omega), digits = 5)  # What is the mode, median, and expectation of X_1?
```

> - The MODE is the element of $\Omega$ with the highest probability ($10$ here)
> - The MEDIAN is the smallest element of $\Omega$ such that AT LEAST 
half of the cumulative probability is less than or equal to that element ($8$ here)
> - EXPECTATION of a discrete random variable $X$ is defined as
$$\mathbb{E}X = \sum_{x\in\Omega}\left[x\times\Pr\left(x\right)\right] \equiv \mu$$
> - Expectation is just a probability-weighted sum ($7.043$ here)

## Practice Problems

> - How would we calculate the expectation of the second roll given that $x_1 = 7$
  pins were knocked down on the first roll? 
> - Answer: `sum(Omega * Pr(Omega, n = 10 - 7))`, which is `r sum(Omega * Pr(Omega, n = 10 - 7))`
> - How would we calculate the expectation of the second roll in a frame of bowling?

## Marginal Expectation of Second Roll in Bowling {.build}

$$\begin{eqnarray*}
\mathbb{E}X_{2} & = & \sum_{x_{j} = 0}^{10}  x_{j}\Pr\left(X_{2} = x_{j}\right) \\
                & = & \sum_{x_{j} = 0}^{10}  x_{j} \sum_{x_{i} = 0}^{10}
                      \Pr\left(x_{i} \bigcap x_{j} \mid n = 10\right) \\
                & = & \sum_{x_{j} = 0}^{10}  x_{j} \sum_{x_{i} = 0}^{10}
\Pr\left(x_{i}\mid n = 10\right) \Pr\left(x_{j}\mid n = 10 - x_i\right)
\end{eqnarray*}$$

```{r}
Pr_X2 <- colSums(joint_Pr) # marginal probabilities of the second roll
EX2 <- sum(Omega * Pr_X2)  # definition of marginal expectation
EX2
```

## Expectations of Functions of Discrete RVs

- Let $g\left(X\right)$ be a function of a discrete random variable whose expectation is
$$\mathbb{E}g\left(X\right) = \sum_{x\in\Omega}\left[g\left(x\right)\times\Pr\left(x\right)\right]
\neq g\left(\mathbb{E}X\right)$$
- It is often sensible to make a decision that maximizes EXPECTED utility:

    1. Enumerate $D$ possible decisions $\{d_1, d_2, \dots, d_D\}$ that are under consideration
    2. Define a utility function $g\left(d,\dots\right)$ that also depends on unknown (and 
      maybe some known) quantities
    3. Obtain / update your conditional probability distribution for all the unknowns given all
      the knowns using Bayes' Rule
    4. Evaluate $\mathbb{E}g\left(d,\dots\right)$ for each of the $D$ decisions
    5. Choose the decision that has the highest value in (4)

## Blackjack

> - How can you play a card game, such as blackjack, if you can't see all the cards?

> - Make the decision to **H**it or **S**tand (or possibly **D**ouble down or **SP**lit)
  to maximize expected profit. That collection of decisions forms a strategy. If we
  assume there are an infinite number of decks in the "shoe", this strategy is called "basic"
  https://en.wikipedia.org/wiki/Blackjack#Basic_strategy
  
> - The pre-hand expectation of the basic strategy is about $-1\%$ of the initial bet

> - After the cards have been dealt, the CONDITIONAL expectation may be $> 0$
  
> - The pre-hand CONDITIONAL expectation of "advantage" strategies can become slightly positive 
  if you mentally keep track of the cards that have already been dealt from a finite shoe


## Sampling without Replacement {.build}

- The hypergeometric distribution corresponds to sampling WITHOUT replacement and
  $\Pr\left(\left.x\right|m,n,k\right) = {{m \choose x}{n \choose k - x}} / {{m + n \choose k}}$ where
  
    - $m$ is the number of "good" elements in the set being drawn from
    - $n$ is the number of "bad" elements in the set being drawn from
    - $k$ is the number of times you draw without replacement
    - ${a \choose b} = \frac{a!}{b!\left(a - b\right)!}$ is the choose function, 
      which is defined as zero if $a < b$

> - What is the pre-hand probability of being dealt $21$ in blackjack?
```{r}
dhyper(x = 1, m = 4 * 4, n = 9  * 4, k = 1) * # probability of a 10, J, Q, or K
dhyper(x = 1, m = 4,     n = 51 - 4, k = 1) * # probability of an Ace given a non-Ace already
2                                             # can get the Ace either first or second
```

## Blackjack Continued {.build}

> - What is the probability of being dealt two cards of the same value in blackjack,
  in which case you have the option to split?
```{r}
9 * dhyper(x = 2, m = 4,     n = 12 * 4, k = 2) + # probability of a pair of A, 2, ..., 9
    dhyper(x = 2, m = 4 * 4, n =  9 * 4, k = 2)   # probability of two cards valued at 10
```

> - Suppose you have a 9 and a 2 from an infinite shoe. How would you calculate the probability 
  that you end up winning if you double down against a dealer's 6?
